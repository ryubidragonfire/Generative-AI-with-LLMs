{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fine-Tune a Generative AI Model for Dialogue Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will fine-tune an existing LLM from Hugging Face for enhanced dialogue summarization. You will use the [FLAN-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5) model, which provides a high quality instruction tuned model and can summarize text out of the box. To improve the inferences, you will explore a full fine-tuning approach and evaluate the results with ROUGE metrics. Then you will perform Parameter Efficient Fine-Tuning (PEFT), evaluate the resulting model and see that the benefits of PEFT outweigh the slightly-lower performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- [ 1 - Set up Kernel, Load Required Dependencies, Dataset and LLM](#1)\n",
    "  - [ 1.1 - Set up Kernel and Required Dependencies](#1.1)\n",
    "  - [ 1.2 - Load Dataset and LLM](#1.2)\n",
    "  - [ 1.3 - Test the Model with Zero Shot Inferencing](#1.3)\n",
    "- [ 2 - Perform Full Fine-Tuning](#2)\n",
    "  - [ 2.1 - Preprocess the Dialog-Summary Dataset](#2.1)\n",
    "  - [ 2.2 - Fine-Tune the Model with the Preprocessed Dataset](#2.2)\n",
    "  - [ 2.3 - Evaluate the Model Qualitatively (Human Evaluation)](#2.3)\n",
    "  - [ 2.4 - Evaluate the Model Quantitatively (with ROUGE Metric)](#2.4)\n",
    "- [ 3 - Perform Parameter Efficient Fine-Tuning (PEFT)](#3)\n",
    "  - [ 3.1 - Setup the PEFT/LoRA model for Fine-Tuning](#3.1)\n",
    "  - [ 3.2 - Train PEFT Adapter](#3.2)\n",
    "  - [ 3.3 - Evaluate the Model Qualitatively (Human Evaluation)](#3.3)\n",
    "  - [ 3.4 - Evaluate the Model Quantitatively (with ROUGE Metric)](#3.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Set up Kernel, Load Required Dependencies, Dataset and LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1.1'></a>\n",
    "### 1.1 - Set up Kernel and Required Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "To begin with, check that the kernel is selected correctly.\n",
    "\n",
    "<img src=\"images/kernel_set_up.png\" width=\"300\"/>\n",
    "\n",
    "If you click on that (top right of the screen), you'll be able to see and check the details of the image, kernel, and instance type.\n",
    "\n",
    "<img src=\"images/w2_kernel_and_instance_type.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now install the required packages for the LLM and datasets.\n",
    "\n",
    "<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSIxMjUiIHZpZXdCb3g9IjAgMCA4MDAgMTI1IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogICAgPGRlZnM+CiAgICAgICAgPGxpbmVhckdyYWRpZW50IGlkPSJmYWRlR3JhZGllbnQiIHgxPSIwIiB4Mj0iMSI+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMCUiIHN0b3AtY29sb3I9IiNGMEYwRjAiLz4KICAgICAgICAgICAgPHN0b3Agb2Zmc2V0PSIxMDAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIiBzdG9wLW9wYWNpdHk9IjAiLz4KICAgICAgICA8L2xpbmVhckdyYWRpZW50PgogICAgICAgIDxtYXNrIGlkPSJmYWRlTWFzayI+CiAgICAgICAgICAgIDxyZWN0IHg9IjAiIHk9IjAiIHdpZHRoPSI3NTAiIGhlaWdodD0iMTI1IiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSIxMjUiIGZpbGw9InVybCgjZmFkZUdyYWRpZW50KSIvPgogICAgICAgIDwvbWFzaz4KICAgIDwvZGVmcz4KICAgIDxwYXRoIGQ9Ik0zLDUwIEE1MCw1MCAwIDAgMSA1MywzIEw3OTcsMyBMNzk3LDk3IEw5Nyw5NyBMNTAsMTE1IEwzLDk3IFoiIGZpbGw9IiNGMEYwRjAiIHN0cm9rZT0iI0UwRTBFMCIgc3Ryb2tlLXdpZHRoPSIxIiBtYXNrPSJ1cmwoI2ZhZGVNYXNrKSIvPgogICAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgcj0iMzAiIGZpbGw9IiM1N2M0ZjgiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIxIi8+CiAgICA8Y2lyY2xlIGN4PSI1MCIgY3k9IjUwIiByPSIyNSIgZmlsbD0iI0YwRjBGMCIvPgogICAgPGxpbmUgeDE9IjUwIiB5MT0iNTAiIHgyPSI1MCIgeTI9IjMwIiBzdHJva2U9IiM1N2M0ZjgiIHN0cm9rZS13aWR0aD0iMyIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIi8+CiAgICA8bGluZSB4MT0iNTAiIHkxPSI1MCIgeDI9IjY1IiB5Mj0iNTAiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIzIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KICAgIDx0ZXh0IHg9IjEwMCIgeT0iMzQiIGZvbnQtZmFtaWx5PSJBcmlhbCwgc2Fucy1zZXJpZiIgZm9udC1zaXplPSIxNCIgZmlsbD0iIzMzMzMzMyI+VGhlIG5leHQgY2VsbCBtYXkgdGFrZSBhIGZldyBtaW51dGVzIHRvIHJ1bi4gUGxlYXNlIGJlIHBhdGllbnQuPC90ZXh0PgogICAgPHRleHQgeD0iMTAwIiB5PSI1NiIgZm9udC1mYW1pbHk9IkFyaWFsLCBzYW5zLXNlcmlmIiBmb250LXNpemU9IjE0IiBmaWxsPSIjMzMzMzMzIj5JZ25vcmUgdGhlIHdhcm5pbmdzIGFuZCBlcnJvcnMsIGFsb25nIHdpdGggdGhlIG5vdGUgYWJvdXQgcmVzdGFydGluZyB0aGUga2VybmVsIGF0IHRoZSBlbmQuPC90ZXh0Pgo8L3N2Zz4K\" alt=\"Time alert open medium\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \\\n",
    "    evaluate==0.4.0 \\\n",
    "    rouge_score==0.1.2 \\\n",
    "    loralib==0.1.1 \\\n",
    "    peft==0.3.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: evaluate\n",
      "Version: 0.4.0\n",
      "Summary: HuggingFace community-driven open-source library of evaluation\n",
      "Home-page: https://github.com/huggingface/evaluate\n",
      "Author: HuggingFace Inc.\n",
      "Author-email: leandro@huggingface.co\n",
      "License: Apache 2.0\n",
      "Location: /anaconda/envs/azureml_py310_sdkv2_generative_ai_with_llms/lib/python3.10/site-packages\n",
      "Requires: datasets, dill, fsspec, huggingface-hub, multiprocess, numpy, packaging, pandas, requests, responses, tqdm, xxhash\n",
      "Required-by: \n",
      "---\n",
      "Name: rouge-score\n",
      "Version: 0.1.2\n",
      "Summary: Pure python implementation of ROUGE-1.5.5.\n",
      "Home-page: https://github.com/google-research/google-research/tree/master/rouge\n",
      "Author: Google LLC\n",
      "Author-email: rouge-opensource@google.com\n",
      "License: \n",
      "Location: /anaconda/envs/azureml_py310_sdkv2_generative_ai_with_llms/lib/python3.10/site-packages\n",
      "Requires: absl-py, nltk, numpy, six\n",
      "Required-by: \n",
      "---\n",
      "Name: loralib\n",
      "Version: 0.1.1\n",
      "Summary: PyTorch implementation of low-rank adaptation (LoRA), a parameter-efficient approach to adapt a large pre-trained deep learning model which obtains performance on-par with full model fine-tuning.\n",
      "Home-page: https://github.com/microsoft/LoRA\n",
      "Author: Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen\n",
      "Author-email: edward.hu@microsoft.com\n",
      "License: UNKNOWN\n",
      "Location: /anaconda/envs/azureml_py310_sdkv2_generative_ai_with_llms/lib/python3.10/site-packages\n",
      "Requires: \n",
      "Required-by: \n",
      "---\n",
      "Name: peft\n",
      "Version: 0.3.0\n",
      "Summary: Parameter-Efficient Fine-Tuning (PEFT)\n",
      "Home-page: https://github.com/huggingface/peft\n",
      "Author: The HuggingFace team\n",
      "Author-email: sourab@huggingface.co\n",
      "License: Apache\n",
      "Location: /anaconda/envs/azureml_py310_sdkv2_generative_ai_with_llms/lib/python3.10/site-packages\n",
      "Requires: accelerate, numpy, packaging, psutil, pyyaml, torch, transformers\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip show evaluate rouge_score loralib peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.7/site-packages (23.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pytest-astropy 0.8.0 requires pytest-cov>=2.0, which is not installed.\n",
      "pytest-astropy 0.8.0 requires pytest-filter-subpackage>=0.1, which is not installed.\n",
      "spyder 4.0.1 requires pyqt5<5.13; python_version >= \"3\", which is not installed.\n",
      "spyder 4.0.1 requires pyqtwebengine<5.13; python_version >= \"3\", which is not installed.\n",
      "sagemaker 2.165.0 requires importlib-metadata<5.0,>=1.4.0, but you have importlib-metadata 6.6.0 which is incompatible.\n",
      "sparkmagic 0.20.4 requires nest-asyncio==1.5.5, but you have nest-asyncio 1.5.6 which is incompatible.\n",
      "spyder 4.0.1 requires jedi==0.14.1, but you have jedi 0.18.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install --disable-pip-version-check \\\n",
    "    torch==1.13.1 \\\n",
    "    torchdata==0.5.1 --quiet\n",
    "\n",
    "%pip install \\\n",
    "    transformers==4.27.2 \\\n",
    "    datasets==2.11.0 \\\n",
    "    evaluate==0.4.0 \\\n",
    "    rouge_score==0.1.2 \\\n",
    "    loralib==0.1.1 \\\n",
    "    peft==0.3.0 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSI1MCIgdmlld0JveD0iMCAwIDgwMCA1MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxkZWZzPgogICAgICAgIDxsaW5lYXJHcmFkaWVudCBpZD0iZmFkZUdyYWRpZW50IiB4MT0iMCIgeDI9IjEiPgogICAgICAgICAgICA8c3RvcCBvZmZzZXQ9IjAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIi8+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMTAwJSIgc3RvcC1jb2xvcj0iI0YwRjBGMCIgc3RvcC1vcGFjaXR5PSIwIi8+CiAgICAgICAgPC9saW5lYXJHcmFkaWVudD4KICAgICAgICA8bWFzayBpZD0iZmFkZU1hc2siPgogICAgICAgICAgICA8cmVjdCB4PSIwIiB5PSIwIiB3aWR0aD0iNzUwIiBoZWlnaHQ9IjUwIiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSI1MCIgZmlsbD0idXJsKCNmYWRlR3JhZGllbnQpIi8+CiAgICAgICAgPC9tYXNrPgogICAgPC9kZWZzPgogICAgPHBhdGggZD0iTTI1LDUwIFEwLDUwIDAsMjUgTDUwLDMgTDk3LDI1IEw3OTcsMjUgTDc5Nyw1MCBMMjUsNTAgWiIgZmlsbD0iI0YwRjBGMCIgc3Ryb2tlPSIjRTBFMEUwIiBzdHJva2Utd2lkdGg9IjEiIG1hc2s9InVybCgjZmFkZU1hc2spIi8+Cjwvc3ZnPgo=\" alt=\"Time alert close\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Import the necessary components. Some of them are new for this week, they will be discussed later in the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='1.2'></a>\n",
    "### 1.2 - Load Dataset and LLM\n",
    "\n",
    "You are going to continue experimenting with the [DialogSum](https://huggingface.co/datasets/knkarthick/dialogsum) Hugging Face dataset. It contains 10,000+ dialogues with the corresponding manually labeled summaries and topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/azureuser/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9866bb611d5a4011a256d40cb6a23776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "\n",
    "dataset = load_dataset(huggingface_dataset_name)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Load the pre-trained [FLAN-T5 model](https://huggingface.co/docs/transformers/model_doc/flan-t5) and its tokenizer directly from HuggingFace. Notice that you will be using the [small version](https://huggingface.co/google/flan-t5-base) of FLAN-T5. Setting `torch_dtype=torch.bfloat16` specifies the memory type to be used by this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name='google/flan-t5-base'\n",
    "\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "It is possible to pull out the number of model parameters and find out how many of them are trainable. The following function can be used to do that, at this stage, you do not need to go into details of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared.weight\n",
      "24674304\n",
      "encoder.block.0.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "encoder.block.0.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "encoder.block.0.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "encoder.block.0.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight\n",
      "384\n",
      "encoder.block.0.layer.0.layer_norm.weight\n",
      "768\n",
      "encoder.block.0.layer.1.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "encoder.block.0.layer.1.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "encoder.block.0.layer.1.DenseReluDense.wo.weight\n",
      "1572864\n",
      "encoder.block.0.layer.1.layer_norm.weight\n",
      "768\n",
      "encoder.block.1.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "encoder.block.1.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "encoder.block.1.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "encoder.block.1.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "encoder.block.1.layer.0.layer_norm.weight\n",
      "768\n",
      "encoder.block.1.layer.1.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "encoder.block.1.layer.1.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "encoder.block.1.layer.1.DenseReluDense.wo.weight\n",
      "1572864\n",
      "encoder.block.1.layer.1.layer_norm.weight\n",
      "768\n",
      "encoder.block.2.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "encoder.block.2.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "encoder.block.2.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "encoder.block.2.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "encoder.block.2.layer.0.layer_norm.weight\n",
      "768\n",
      "encoder.block.2.layer.1.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "encoder.block.2.layer.1.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "encoder.block.2.layer.1.DenseReluDense.wo.weight\n",
      "1572864\n",
      "encoder.block.2.layer.1.layer_norm.weight\n",
      "768\n",
      "encoder.block.3.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "encoder.block.3.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "encoder.block.3.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "encoder.block.3.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "encoder.block.3.layer.0.layer_norm.weight\n",
      "768\n",
      "encoder.block.3.layer.1.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "encoder.block.3.layer.1.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "encoder.block.3.layer.1.DenseReluDense.wo.weight\n",
      "1572864\n",
      "encoder.block.3.layer.1.layer_norm.weight\n",
      "768\n",
      "encoder.block.4.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "encoder.block.4.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "encoder.block.4.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "encoder.block.4.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "encoder.block.4.layer.0.layer_norm.weight\n",
      "768\n",
      "encoder.block.4.layer.1.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "encoder.block.4.layer.1.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "encoder.block.4.layer.1.DenseReluDense.wo.weight\n",
      "1572864\n",
      "encoder.block.4.layer.1.layer_norm.weight\n",
      "768\n",
      "encoder.block.5.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "encoder.block.5.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "encoder.block.5.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "encoder.block.5.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "encoder.block.5.layer.0.layer_norm.weight\n",
      "768\n",
      "encoder.block.5.layer.1.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "encoder.block.5.layer.1.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "encoder.block.5.layer.1.DenseReluDense.wo.weight\n",
      "1572864\n",
      "encoder.block.5.layer.1.layer_norm.weight\n",
      "768\n",
      "encoder.block.6.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "encoder.block.6.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "encoder.block.6.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "encoder.block.6.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "encoder.block.6.layer.0.layer_norm.weight\n",
      "768\n",
      "encoder.block.6.layer.1.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "encoder.block.6.layer.1.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "encoder.block.6.layer.1.DenseReluDense.wo.weight\n",
      "1572864\n",
      "encoder.block.6.layer.1.layer_norm.weight\n",
      "768\n",
      "encoder.block.7.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "encoder.block.7.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "encoder.block.7.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "encoder.block.7.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "encoder.block.7.layer.0.layer_norm.weight\n",
      "768\n",
      "encoder.block.7.layer.1.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "encoder.block.7.layer.1.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "encoder.block.7.layer.1.DenseReluDense.wo.weight\n",
      "1572864\n",
      "encoder.block.7.layer.1.layer_norm.weight\n",
      "768\n",
      "encoder.block.8.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "encoder.block.8.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "encoder.block.8.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "encoder.block.8.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "encoder.block.8.layer.0.layer_norm.weight\n",
      "768\n",
      "encoder.block.8.layer.1.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "encoder.block.8.layer.1.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "encoder.block.8.layer.1.DenseReluDense.wo.weight\n",
      "1572864\n",
      "encoder.block.8.layer.1.layer_norm.weight\n",
      "768\n",
      "encoder.block.9.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "encoder.block.9.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "encoder.block.9.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "encoder.block.9.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "encoder.block.9.layer.0.layer_norm.weight\n",
      "768\n",
      "encoder.block.9.layer.1.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "encoder.block.9.layer.1.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "encoder.block.9.layer.1.DenseReluDense.wo.weight\n",
      "1572864\n",
      "encoder.block.9.layer.1.layer_norm.weight\n",
      "768\n",
      "encoder.block.10.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "encoder.block.10.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "encoder.block.10.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "encoder.block.10.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "encoder.block.10.layer.0.layer_norm.weight\n",
      "768\n",
      "encoder.block.10.layer.1.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "encoder.block.10.layer.1.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "encoder.block.10.layer.1.DenseReluDense.wo.weight\n",
      "1572864\n",
      "encoder.block.10.layer.1.layer_norm.weight\n",
      "768\n",
      "encoder.block.11.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "encoder.block.11.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "encoder.block.11.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "encoder.block.11.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "encoder.block.11.layer.0.layer_norm.weight\n",
      "768\n",
      "encoder.block.11.layer.1.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "encoder.block.11.layer.1.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "encoder.block.11.layer.1.DenseReluDense.wo.weight\n",
      "1572864\n",
      "encoder.block.11.layer.1.layer_norm.weight\n",
      "768\n",
      "encoder.final_layer_norm.weight\n",
      "768\n",
      "decoder.block.0.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "decoder.block.0.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "decoder.block.0.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "decoder.block.0.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight\n",
      "384\n",
      "decoder.block.0.layer.0.layer_norm.weight\n",
      "768\n",
      "decoder.block.0.layer.1.EncDecAttention.q.weight\n",
      "589824\n",
      "decoder.block.0.layer.1.EncDecAttention.k.weight\n",
      "589824\n",
      "decoder.block.0.layer.1.EncDecAttention.v.weight\n",
      "589824\n",
      "decoder.block.0.layer.1.EncDecAttention.o.weight\n",
      "589824\n",
      "decoder.block.0.layer.1.layer_norm.weight\n",
      "768\n",
      "decoder.block.0.layer.2.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "decoder.block.0.layer.2.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "decoder.block.0.layer.2.DenseReluDense.wo.weight\n",
      "1572864\n",
      "decoder.block.0.layer.2.layer_norm.weight\n",
      "768\n",
      "decoder.block.1.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "decoder.block.1.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "decoder.block.1.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "decoder.block.1.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "decoder.block.1.layer.0.layer_norm.weight\n",
      "768\n",
      "decoder.block.1.layer.1.EncDecAttention.q.weight\n",
      "589824\n",
      "decoder.block.1.layer.1.EncDecAttention.k.weight\n",
      "589824\n",
      "decoder.block.1.layer.1.EncDecAttention.v.weight\n",
      "589824\n",
      "decoder.block.1.layer.1.EncDecAttention.o.weight\n",
      "589824\n",
      "decoder.block.1.layer.1.layer_norm.weight\n",
      "768\n",
      "decoder.block.1.layer.2.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "decoder.block.1.layer.2.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "decoder.block.1.layer.2.DenseReluDense.wo.weight\n",
      "1572864\n",
      "decoder.block.1.layer.2.layer_norm.weight\n",
      "768\n",
      "decoder.block.2.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "decoder.block.2.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "decoder.block.2.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "decoder.block.2.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "decoder.block.2.layer.0.layer_norm.weight\n",
      "768\n",
      "decoder.block.2.layer.1.EncDecAttention.q.weight\n",
      "589824\n",
      "decoder.block.2.layer.1.EncDecAttention.k.weight\n",
      "589824\n",
      "decoder.block.2.layer.1.EncDecAttention.v.weight\n",
      "589824\n",
      "decoder.block.2.layer.1.EncDecAttention.o.weight\n",
      "589824\n",
      "decoder.block.2.layer.1.layer_norm.weight\n",
      "768\n",
      "decoder.block.2.layer.2.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "decoder.block.2.layer.2.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "decoder.block.2.layer.2.DenseReluDense.wo.weight\n",
      "1572864\n",
      "decoder.block.2.layer.2.layer_norm.weight\n",
      "768\n",
      "decoder.block.3.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "decoder.block.3.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "decoder.block.3.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "decoder.block.3.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "decoder.block.3.layer.0.layer_norm.weight\n",
      "768\n",
      "decoder.block.3.layer.1.EncDecAttention.q.weight\n",
      "589824\n",
      "decoder.block.3.layer.1.EncDecAttention.k.weight\n",
      "589824\n",
      "decoder.block.3.layer.1.EncDecAttention.v.weight\n",
      "589824\n",
      "decoder.block.3.layer.1.EncDecAttention.o.weight\n",
      "589824\n",
      "decoder.block.3.layer.1.layer_norm.weight\n",
      "768\n",
      "decoder.block.3.layer.2.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "decoder.block.3.layer.2.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "decoder.block.3.layer.2.DenseReluDense.wo.weight\n",
      "1572864\n",
      "decoder.block.3.layer.2.layer_norm.weight\n",
      "768\n",
      "decoder.block.4.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "decoder.block.4.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "decoder.block.4.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "decoder.block.4.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "decoder.block.4.layer.0.layer_norm.weight\n",
      "768\n",
      "decoder.block.4.layer.1.EncDecAttention.q.weight\n",
      "589824\n",
      "decoder.block.4.layer.1.EncDecAttention.k.weight\n",
      "589824\n",
      "decoder.block.4.layer.1.EncDecAttention.v.weight\n",
      "589824\n",
      "decoder.block.4.layer.1.EncDecAttention.o.weight\n",
      "589824\n",
      "decoder.block.4.layer.1.layer_norm.weight\n",
      "768\n",
      "decoder.block.4.layer.2.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "decoder.block.4.layer.2.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "decoder.block.4.layer.2.DenseReluDense.wo.weight\n",
      "1572864\n",
      "decoder.block.4.layer.2.layer_norm.weight\n",
      "768\n",
      "decoder.block.5.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "decoder.block.5.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "decoder.block.5.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "decoder.block.5.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "decoder.block.5.layer.0.layer_norm.weight\n",
      "768\n",
      "decoder.block.5.layer.1.EncDecAttention.q.weight\n",
      "589824\n",
      "decoder.block.5.layer.1.EncDecAttention.k.weight\n",
      "589824\n",
      "decoder.block.5.layer.1.EncDecAttention.v.weight\n",
      "589824\n",
      "decoder.block.5.layer.1.EncDecAttention.o.weight\n",
      "589824\n",
      "decoder.block.5.layer.1.layer_norm.weight\n",
      "768\n",
      "decoder.block.5.layer.2.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "decoder.block.5.layer.2.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "decoder.block.5.layer.2.DenseReluDense.wo.weight\n",
      "1572864\n",
      "decoder.block.5.layer.2.layer_norm.weight\n",
      "768\n",
      "decoder.block.6.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "decoder.block.6.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "decoder.block.6.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "decoder.block.6.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "decoder.block.6.layer.0.layer_norm.weight\n",
      "768\n",
      "decoder.block.6.layer.1.EncDecAttention.q.weight\n",
      "589824\n",
      "decoder.block.6.layer.1.EncDecAttention.k.weight\n",
      "589824\n",
      "decoder.block.6.layer.1.EncDecAttention.v.weight\n",
      "589824\n",
      "decoder.block.6.layer.1.EncDecAttention.o.weight\n",
      "589824\n",
      "decoder.block.6.layer.1.layer_norm.weight\n",
      "768\n",
      "decoder.block.6.layer.2.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "decoder.block.6.layer.2.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "decoder.block.6.layer.2.DenseReluDense.wo.weight\n",
      "1572864\n",
      "decoder.block.6.layer.2.layer_norm.weight\n",
      "768\n",
      "decoder.block.7.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "decoder.block.7.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "decoder.block.7.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "decoder.block.7.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "decoder.block.7.layer.0.layer_norm.weight\n",
      "768\n",
      "decoder.block.7.layer.1.EncDecAttention.q.weight\n",
      "589824\n",
      "decoder.block.7.layer.1.EncDecAttention.k.weight\n",
      "589824\n",
      "decoder.block.7.layer.1.EncDecAttention.v.weight\n",
      "589824\n",
      "decoder.block.7.layer.1.EncDecAttention.o.weight\n",
      "589824\n",
      "decoder.block.7.layer.1.layer_norm.weight\n",
      "768\n",
      "decoder.block.7.layer.2.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "decoder.block.7.layer.2.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "decoder.block.7.layer.2.DenseReluDense.wo.weight\n",
      "1572864\n",
      "decoder.block.7.layer.2.layer_norm.weight\n",
      "768\n",
      "decoder.block.8.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "decoder.block.8.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "decoder.block.8.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "decoder.block.8.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "decoder.block.8.layer.0.layer_norm.weight\n",
      "768\n",
      "decoder.block.8.layer.1.EncDecAttention.q.weight\n",
      "589824\n",
      "decoder.block.8.layer.1.EncDecAttention.k.weight\n",
      "589824\n",
      "decoder.block.8.layer.1.EncDecAttention.v.weight\n",
      "589824\n",
      "decoder.block.8.layer.1.EncDecAttention.o.weight\n",
      "589824\n",
      "decoder.block.8.layer.1.layer_norm.weight\n",
      "768\n",
      "decoder.block.8.layer.2.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "decoder.block.8.layer.2.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "decoder.block.8.layer.2.DenseReluDense.wo.weight\n",
      "1572864\n",
      "decoder.block.8.layer.2.layer_norm.weight\n",
      "768\n",
      "decoder.block.9.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "decoder.block.9.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "decoder.block.9.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "decoder.block.9.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "decoder.block.9.layer.0.layer_norm.weight\n",
      "768\n",
      "decoder.block.9.layer.1.EncDecAttention.q.weight\n",
      "589824\n",
      "decoder.block.9.layer.1.EncDecAttention.k.weight\n",
      "589824\n",
      "decoder.block.9.layer.1.EncDecAttention.v.weight\n",
      "589824\n",
      "decoder.block.9.layer.1.EncDecAttention.o.weight\n",
      "589824\n",
      "decoder.block.9.layer.1.layer_norm.weight\n",
      "768\n",
      "decoder.block.9.layer.2.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "decoder.block.9.layer.2.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "decoder.block.9.layer.2.DenseReluDense.wo.weight\n",
      "1572864\n",
      "decoder.block.9.layer.2.layer_norm.weight\n",
      "768\n",
      "decoder.block.10.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "decoder.block.10.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "decoder.block.10.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "decoder.block.10.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "decoder.block.10.layer.0.layer_norm.weight\n",
      "768\n",
      "decoder.block.10.layer.1.EncDecAttention.q.weight\n",
      "589824\n",
      "decoder.block.10.layer.1.EncDecAttention.k.weight\n",
      "589824\n",
      "decoder.block.10.layer.1.EncDecAttention.v.weight\n",
      "589824\n",
      "decoder.block.10.layer.1.EncDecAttention.o.weight\n",
      "589824\n",
      "decoder.block.10.layer.1.layer_norm.weight\n",
      "768\n",
      "decoder.block.10.layer.2.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "decoder.block.10.layer.2.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "decoder.block.10.layer.2.DenseReluDense.wo.weight\n",
      "1572864\n",
      "decoder.block.10.layer.2.layer_norm.weight\n",
      "768\n",
      "decoder.block.11.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "decoder.block.11.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "decoder.block.11.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "decoder.block.11.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "decoder.block.11.layer.0.layer_norm.weight\n",
      "768\n",
      "decoder.block.11.layer.1.EncDecAttention.q.weight\n",
      "589824\n",
      "decoder.block.11.layer.1.EncDecAttention.k.weight\n",
      "589824\n",
      "decoder.block.11.layer.1.EncDecAttention.v.weight\n",
      "589824\n",
      "decoder.block.11.layer.1.EncDecAttention.o.weight\n",
      "589824\n",
      "decoder.block.11.layer.1.layer_norm.weight\n",
      "768\n",
      "decoder.block.11.layer.2.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "decoder.block.11.layer.2.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "decoder.block.11.layer.2.DenseReluDense.wo.weight\n",
      "1572864\n",
      "decoder.block.11.layer.2.layer_norm.weight\n",
      "768\n",
      "decoder.final_layer_norm.weight\n",
      "768\n",
      "lm_head.weight\n",
      "24674304\n",
      "trainable model parameters: 247577856\n",
      "all model parameters: 247577856\n",
      "percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    \n",
    "    for weight_name, param in model.named_parameters():\n",
    "        print(weight_name)\n",
    "        print(param.numel())\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='1.3'></a>\n",
    "### 1.3 - Test the Model with Zero Shot Inferencing\n",
    "\n",
    "Test the model with the zero shot inferencing. You can see that the model struggles to summarize the dialogue compared to the baseline summary, but it does pull out some important information from the text which indicates the model can be fine-tuned to the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "Summary:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "#Person1#: I'm thinking of upgrading my computer.\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    original_model.generate(\n",
    "        inputs[\"input_ids\"], \n",
    "        max_new_tokens=200,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Perform Full Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='2.1'></a>\n",
    "### 2.1 - Preprocess the Dialog-Summary Dataset\n",
    "\n",
    "You need to convert the dialog-summary (prompt-response) pairs into explicit instructions for the LLM. Prepend an instruction to the start of the dialog with `Summarize the following conversation` and to the start of the summary with `Summary` as follows:\n",
    "\n",
    "Training prompt (dialogue):\n",
    "```\n",
    "Summarize the following conversation.\n",
    "\n",
    "    Chris: This is his part of the conversation.\n",
    "    Antje: This is her part of the conversation.\n",
    "    \n",
    "Summary: \n",
    "```\n",
    "\n",
    "Training response (summary):\n",
    "```\n",
    "Both Chris and Antje participated in the conversation.\n",
    "```\n",
    "\n",
    "Then preprocess the prompt-response dataset into tokens and pull out their `input_ids` (1 per token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/azureuser/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-f1d0a168d401bac5.arrow\n",
      "Loading cached processed dataset at /home/azureuser/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-995b5a9f2efe9f9e.arrow\n",
      "Loading cached processed dataset at /home/azureuser/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-1d7bda631b80c31e.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train', 'test', 'validation'])\n",
      "{'train': ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'labels'], 'test': ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'labels'], 'validation': ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'labels']}\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'labels'],\n",
      "        num_rows: 12460\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'labels'],\n",
      "        num_rows: 1500\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'labels'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "})\n",
      "{'train': ['input_ids', 'labels'], 'test': ['input_ids', 'labels'], 'validation': ['input_ids', 'labels']}\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 12460\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 1500\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': (12460, 2), 'test': (1500, 2), 'validation': (500, 2)}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    start_prompt = 'Summarize the following conversation.\\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
    "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    return example\n",
    "\n",
    "\n",
    "# The dataset actually contains 3 diff splits: train, validation, test.\n",
    "# The tokenize_function code is handling all data across all splits in batches.\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "print(tokenized_datasets.keys())\n",
    "print(tokenized_datasets.column_names)\n",
    "print(tokenized_datasets)\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary',])\n",
    "print(tokenized_datasets.column_names)\n",
    "print(tokenized_datasets)\n",
    "tokenized_datasets.shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "To save some time in the lab, you will subsample the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/azureuser/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-30e1c838968e58ed.arrow\n",
      "Loading cached processed dataset at /home/azureuser/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-80a9b8ad62390d5a.arrow\n",
      "Loading cached processed dataset at /home/azureuser/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-46a005b42eea80c8.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': (125, 2), 'test': (15, 2), 'validation': (5, 2)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)\n",
    "tokenized_datasets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Check the shapes of all three parts of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Training: (125, 2)\n",
      "Validation: (5, 2)\n",
      "Test: (15, 2)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 125\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 15\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 5\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
    "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
    "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
    "\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output dataset is ready for fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.2'></a>\n",
    "### 2.2 - Fine-Tune the Model with the Preprocessed Dataset\n",
    "\n",
    "Now utilize the built-in Hugging Face `Trainer` class (see the documentation [here](https://huggingface.co/docs/transformers/main_classes/trainer)). Pass the preprocessed dataset with reference to the original model. Other training parameters are found experimentally and there is no need to go into details about those at the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dialogue-summary-training-1698665466\n"
     ]
    }
   ],
   "source": [
    "output_dir = f'./dialogue-summary-training-{str(int(time.time()))}'\n",
    "print(output_dir)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    max_steps=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=original_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Start training process...\n",
    "\n",
    "<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSIxMjUiIHZpZXdCb3g9IjAgMCA4MDAgMTI1IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogICAgPGRlZnM+CiAgICAgICAgPGxpbmVhckdyYWRpZW50IGlkPSJmYWRlR3JhZGllbnQiIHgxPSIwIiB4Mj0iMSI+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMCUiIHN0b3AtY29sb3I9IiNGMEYwRjAiLz4KICAgICAgICAgICAgPHN0b3Agb2Zmc2V0PSIxMDAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIiBzdG9wLW9wYWNpdHk9IjAiLz4KICAgICAgICA8L2xpbmVhckdyYWRpZW50PgogICAgICAgIDxtYXNrIGlkPSJmYWRlTWFzayI+CiAgICAgICAgICAgIDxyZWN0IHg9IjAiIHk9IjAiIHdpZHRoPSI3NTAiIGhlaWdodD0iMTI1IiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSIxMjUiIGZpbGw9InVybCgjZmFkZUdyYWRpZW50KSIvPgogICAgICAgIDwvbWFzaz4KICAgIDwvZGVmcz4KICAgIDxwYXRoIGQ9Ik0zLDUwIEE1MCw1MCAwIDAgMSA1MywzIEw3OTcsMyBMNzk3LDk3IEw5Nyw5NyBMNTAsMTE1IEwzLDk3IFoiIGZpbGw9IiNGMEYwRjAiIHN0cm9rZT0iI0UwRTBFMCIgc3Ryb2tlLXdpZHRoPSIxIiBtYXNrPSJ1cmwoI2ZhZGVNYXNrKSIvPgogICAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgcj0iMzAiIGZpbGw9IiM1N2M0ZjgiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIxIi8+CiAgICA8Y2lyY2xlIGN4PSI1MCIgY3k9IjUwIiByPSIyNSIgZmlsbD0iI0YwRjBGMCIvPgogICAgPGxpbmUgeDE9IjUwIiB5MT0iNTAiIHgyPSI1MCIgeTI9IjMwIiBzdHJva2U9IiM1N2M0ZjgiIHN0cm9rZS13aWR0aD0iMyIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIi8+CiAgICA8bGluZSB4MT0iNTAiIHkxPSI1MCIgeDI9IjY1IiB5Mj0iNTAiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIzIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KICAgIDx0ZXh0IHg9IjEwMCIgeT0iMzQiIGZvbnQtZmFtaWx5PSJBcmlhbCwgc2Fucy1zZXJpZiIgZm9udC1zaXplPSIxNCIgZmlsbD0iIzMzMzMzMyI+VGhlIG5leHQgY2VsbCBtYXkgdGFrZSBhIGZldyBtaW51dGVzIHRvIHJ1bi4gUGxlYXNlIGJlIHBhdGllbnQuPC90ZXh0PgogICAgPHRleHQgeD0iMTAwIiB5PSI1NiIgZm9udC1mYW1pbHk9IkFyaWFsLCBzYW5zLXNlcmlmIiBmb250LXNpemU9IjE0IiBmaWxsPSIjMzMzMzMzIj5Zb3UgY2FuIHNhZmVseSBpZ25vcmUgdGhlIHdhcm5pbmcgbWVzc2FnZXMuPC90ZXh0Pgo8L3N2Zz4K\" alt=\"Time alert open medium\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py310_sdkv2_generative_ai_with_llms/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'summarization': {'early_stopping': True, 'length_penalty': 2.0, 'max_length': 200, 'min_length': 30, 'no_repeat_ngram_size': 3, 'num_beams': 4, 'prefix': 'summarize: '}, 'translation_en_to_de': {'early_stopping': True, 'max_length': 300, 'num_beams': 4, 'prefix': 'translate English to German: '}, 'translation_en_to_fr': {'early_stopping': True, 'max_length': 300, 'num_beams': 4, 'prefix': 'translate English to French: '}, 'translation_en_to_ro': {'early_stopping': True, 'max_length': 300, 'num_beams': 4, 'prefix': 'translate English to Romanian: '}}\" for key \"task_specific_params\" as a parameter. MLflow's log_param() only accepts values no longer than 250 characters so we dropped this attribute. You can use `MLFLOW_FLATTEN_PARAMS` environment variable to flatten the parameters and avoid this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:02, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>49.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1, training_loss=49.75, metrics={'train_runtime': 208.3073, 'train_samples_per_second': 0.038, 'train_steps_per_second': 0.005, 'total_flos': 5478058819584.0, 'train_loss': 49.75, 'epoch': 0.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model() # save in output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSI1MCIgdmlld0JveD0iMCAwIDgwMCA1MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxkZWZzPgogICAgICAgIDxsaW5lYXJHcmFkaWVudCBpZD0iZmFkZUdyYWRpZW50IiB4MT0iMCIgeDI9IjEiPgogICAgICAgICAgICA8c3RvcCBvZmZzZXQ9IjAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIi8+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMTAwJSIgc3RvcC1jb2xvcj0iI0YwRjBGMCIgc3RvcC1vcGFjaXR5PSIwIi8+CiAgICAgICAgPC9saW5lYXJHcmFkaWVudD4KICAgICAgICA8bWFzayBpZD0iZmFkZU1hc2siPgogICAgICAgICAgICA8cmVjdCB4PSIwIiB5PSIwIiB3aWR0aD0iNzUwIiBoZWlnaHQ9IjUwIiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSI1MCIgZmlsbD0idXJsKCNmYWRlR3JhZGllbnQpIi8+CiAgICAgICAgPC9tYXNrPgogICAgPC9kZWZzPgogICAgPHBhdGggZD0iTTI1LDUwIFEwLDUwIDAsMjUgTDUwLDMgTDk3LDI1IEw3OTcsMjUgTDc5Nyw1MCBMMjUsNTAgWiIgZmlsbD0iI0YwRjBGMCIgc3Ryb2tlPSIjRTBFMEUwIiBzdHJva2Utd2lkdGg9IjEiIG1hc2s9InVybCgjZmFkZU1hc2spIi8+Cjwvc3ZnPgo=\" alt=\"Time alert close\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a fully fine-tuned version of the model would take a few hours on a GPU. To save time, download a checkpoint of the fully fine-tuned model to use in the rest of this notebook. This fully fine-tuned model will also be referred to as the **instruct model** in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/bin/bash: aws: command not found\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive s3://dlai-generative-ai/models/flan-dialogue-summary-checkpoint/ ./flan-dialogue-summary-checkpoint/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The size of the downloaded instruct model is approximately 1GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 945M May 15 10:25 ./flan-dialogue-summary-checkpoint/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "!ls -alh ./flan-dialogue-summary-checkpoint/pytorch_model.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Create an instance of the `AutoModelForSeq2SeqLM` class for the instruct model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#instruct_model = AutoModelForSeq2SeqLM.from_pretrained(\"./flan-dialogue-summary-checkpoint\", torch_dtype=torch.bfloat16)\n",
    "instruct_model = AutoModelForSeq2SeqLM.from_pretrained(output_dir, torch_dtype=torch.bfloat16)\n",
    "instruct_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.3'></a>\n",
    "### 2.3 - Evaluate the Model Qualitatively (Human Evaluation)\n",
    "\n",
    "As with many GenAI applications, a qualitative approach where you ask yourself the question \"Is my model behaving the way it is supposed to?\" is usually a good starting point. In the example below (the same one we started this notebook with), you can see how the fine-tuned model is able to create a reasonable summary of the dialogue compared to the original inability to understand what is being asked of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "ORIGINAL MODEL:\n",
      "#Person2#: I'm thinking of adding a painting program to your software. #Person2#: I'm thinking of adding a frog. #Person2#: I'm thinking of adding a CD-ROM drive.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INSTRUCT MODEL:\n",
      "#Person1#: I'm thinking of upgrading my computer.\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "human_baseline_summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n",
    "print(dash_line)\n",
    "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.4'></a>\n",
    "### 2.4 - Evaluate the Model Quantitatively (with ROUGE Metric)\n",
    "\n",
    "The [ROUGE metric](https://en.wikipedia.org/wiki/ROUGE_(metric)) helps quantify the validity of summarizations produced by models. It compares summarizations to a \"baseline\" summary which is usually created by a human. While not perfect, it does indicate the overall increase in summarization effectiveness that we have accomplished by fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationModule(name: \"rouge\", module_type: \"metric\", features: [{'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id=None)}, {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')}], usage: \"\"\"\n",
       "Calculates average rouge scores for a list of hypotheses and references\n",
       "Args:\n",
       "    predictions: list of predictions to score. Each prediction\n",
       "        should be a string with tokens separated by spaces.\n",
       "    references: list of reference for each prediction. Each\n",
       "        reference should be a string with tokens separated by spaces.\n",
       "    rouge_types: A list of rouge types to calculate.\n",
       "        Valid names:\n",
       "        `\"rouge{n}\"` (e.g. `\"rouge1\"`, `\"rouge2\"`) where: {n} is the n-gram based scoring,\n",
       "        `\"rougeL\"`: Longest common subsequence based scoring.\n",
       "        `\"rougeLsum\"`: rougeLsum splits text using `\"\n",
       "\"`.\n",
       "        See details in https://github.com/huggingface/datasets/issues/617\n",
       "    use_stemmer: Bool indicating whether Porter stemmer should be used to strip word suffixes.\n",
       "    use_aggregator: Return aggregates if this is set to True\n",
       "Returns:\n",
       "    rouge1: rouge_1 (f1),\n",
       "    rouge2: rouge_2 (f1),\n",
       "    rougeL: rouge_l (f1),\n",
       "    rougeLsum: rouge_lsum (f1)\n",
       "Examples:\n",
       "\n",
       "    >>> rouge = evaluate.load('rouge')\n",
       "    >>> predictions = [\"hello there\", \"general kenobi\"]\n",
       "    >>> references = [\"hello there\", \"general kenobi\"]\n",
       "    >>> results = rouge.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge = evaluate.load('rouge')\n",
    "rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the outputs for the sample of the test dataset (only 10 dialogues and summaries to save time), and save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_baseline_summaries</th>\n",
       "      <th>original_model_summaries</th>\n",
       "      <th>instruct_model_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n",
       "      <td>#Person1: Thank you for your help. #Person2: T...</td>\n",
       "      <td>#Person1#: I need to take a dictation for you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In order to prevent employees from wasting tim...</td>\n",
       "      <td>Employees are to be notified by this afternoon.</td>\n",
       "      <td>#Person1#: I need to take a dictation for you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n",
       "      <td>#Person1#: This memo should go out by this aft...</td>\n",
       "      <td>#Person1#: I need to take a dictation for you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Person2# arrives late because of traffic jam....</td>\n",
       "      <td>The traffic jam at the intersection of the Car...</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n",
       "      <td>The traffic jam is always congested down there.</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#Person2# complains to #Person1# about the tra...</td>\n",
       "      <td>The traffic in the city is always congested.</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#Person1# and Kate talk about the divorce betw...</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#Person1# and Brian are at the birthday party ...</td>\n",
       "      <td>Brian's birthday is coming to Brian's birthday.</td>\n",
       "      <td>#Person1#: Happy birthday, Brian. #Person2#: I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            human_baseline_summaries  \\\n",
       "0  Ms. Dawson helps #Person1# to write a memo to ...   \n",
       "1  In order to prevent employees from wasting tim...   \n",
       "2  Ms. Dawson takes a dictation for #Person1# abo...   \n",
       "3  #Person2# arrives late because of traffic jam....   \n",
       "4  #Person2# decides to follow #Person1#'s sugges...   \n",
       "5  #Person2# complains to #Person1# about the tra...   \n",
       "6  #Person1# tells Kate that Masha and Hero get d...   \n",
       "7  #Person1# tells Kate that Masha and Hero are g...   \n",
       "8  #Person1# and Kate talk about the divorce betw...   \n",
       "9  #Person1# and Brian are at the birthday party ...   \n",
       "\n",
       "                            original_model_summaries  \\\n",
       "0  #Person1: Thank you for your help. #Person2: T...   \n",
       "1    Employees are to be notified by this afternoon.   \n",
       "2  #Person1#: This memo should go out by this aft...   \n",
       "3  The traffic jam at the intersection of the Car...   \n",
       "4    The traffic jam is always congested down there.   \n",
       "5       The traffic in the city is always congested.   \n",
       "6               Masha and Hero are getting divorced.   \n",
       "7               Masha and Hero are getting divorced.   \n",
       "8               Masha and Hero are getting divorced.   \n",
       "9    Brian's birthday is coming to Brian's birthday.   \n",
       "\n",
       "                            instruct_model_summaries  \n",
       "0     #Person1#: I need to take a dictation for you.  \n",
       "1     #Person1#: I need to take a dictation for you.  \n",
       "2     #Person1#: I need to take a dictation for you.  \n",
       "3  The traffic jam at the Carrefour intersection ...  \n",
       "4  The traffic jam at the Carrefour intersection ...  \n",
       "5  The traffic jam at the Carrefour intersection ...  \n",
       "6               Masha and Hero are getting divorced.  \n",
       "7               Masha and Hero are getting divorced.  \n",
       "8               Masha and Hero are getting divorced.  \n",
       "9  #Person1#: Happy birthday, Brian. #Person2#: I...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogues = dataset['test'][0:10]['dialogue']\n",
    "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
    "\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "\n",
    "for _, dialogue in enumerate(dialogues):\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary: \"\"\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "\n",
    "    instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "    instruct_model_summaries.append(instruct_model_text_output)\n",
    "    \n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries))\n",
    " \n",
    "df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Evaluate the models computing ROUGE metrics. Notice the improvement in the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.23584347984347984, 'rouge2': 0.08681987577639752, 'rougeL': 0.19598244398244397, 'rougeLsum': 0.1996968156968157}\n",
      "INSTRUCT MODEL:\n",
      "{'rouge1': 0.24089921652421653, 'rouge2': 0.11769053708439897, 'rougeL': 0.22001958689458687, 'rougeLsum': 0.22134175465057818}\n"
     ]
    }
   ],
   "source": [
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions=instruct_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)\n",
    "\n",
    "### Notebook original output\n",
    "# ORIGINAL MODEL:\n",
    "# {'rouge1': 0.24223171760013867, 'rouge2': 0.10614243734192583, 'rougeL': 0.21380459196706333, 'rougeLsum': 0.21740921541379205}\n",
    "# INSTRUCT MODEL:\n",
    "# {'rouge1': 0.41026607717457186, 'rouge2': 0.17840645241958838, 'rougeL': 0.2977022096267017, 'rougeLsum': 0.2987374187518165}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file `data/dialogue-summary-training-results.csv` contains a pre-populated list of all model results which you can use to evaluate on a larger section of data. Let's do that for each of the models:\n",
    "\n",
    "###  Two flavors of ROUGE-L\n",
    "In the ROUGE paper, two flavors of ROUGE are described:\n",
    "\n",
    "**sentence-level:** Compute **longest common subsequence (LCS)** between two pieces of text. Newlines are ignored. This is called `rougeL` in this package.\n",
    "\n",
    "**summary-level:** **Newlines** in the text are interpreted as sentence **boundaries**, and the LCS is computed between each pair of reference and candidate sentences, and something called union-LCS is computed. This is called `rougeLsum` in this package. \n",
    "\n",
    "This is the ROUGE-L reported in [Get To The Point: Summarization with Pointer-Generator Networks](https://arxiv.org/abs/1704.04368), for example. If your references/candidates do not have newline delimiters, you can use the --split_summaries flag (or optional argument in `RougeScorer`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.2334158581572823, 'rouge2': 0.07603964187010573, 'rougeL': 0.20145520923859048, 'rougeLsum': 0.20145899339006135}\n",
      "INSTRUCT MODEL:\n",
      "{'rouge1': 0.42161291557556113, 'rouge2': 0.18035380596301792, 'rougeL': 0.3384439349963909, 'rougeLsum': 0.33835653595561666}\n"
     ]
    }
   ],
   "source": [
    "results = pd.read_csv(\"data/dialogue-summary-training-results.csv\")\n",
    "\n",
    "human_baseline_summaries = results['human_baseline_summaries'].values\n",
    "original_model_summaries = results['original_model_summaries'].values\n",
    "instruct_model_summaries = results['instruct_model_summaries'].values\n",
    "\n",
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions=instruct_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The results show substantial improvement in all ROUGE metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute percentage improvement of INSTRUCT MODEL over HUMAN BASELINE\n",
      "rouge1: 18.82%\n",
      "rouge2: 10.43%\n",
      "rougeL: 13.70%\n",
      "rougeLsum: 13.69%\n"
     ]
    }
   ],
   "source": [
    "print(\"Absolute percentage improvement of INSTRUCT MODEL over HUMAN BASELINE\")\n",
    "\n",
    "improvement = (np.array(list(instruct_model_results.values())) - np.array(list(original_model_results.values())))\n",
    "for key, value in zip(instruct_model_results.keys(), improvement):\n",
    "    print(f'{key}: {value*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Perform Parameter Efficient Fine-Tuning (PEFT)\n",
    "\n",
    "Now, let's perform **Parameter Efficient Fine-Tuning (PEFT)** fine-tuning as opposed to \"full fine-tuning\" as you did above. PEFT is a form of instruction fine-tuning that is much more efficient than full fine-tuning - with comparable evaluation results as you will see soon. \n",
    "\n",
    "PEFT is a generic term that includes **Low-Rank Adaptation (LoRA)** and prompt tuning (which is NOT THE SAME as prompt engineering!). In most cases, when someone says PEFT, they typically mean LoRA. LoRA, at a very high level, allows the user to fine-tune their model using fewer compute resources (in some cases, a single GPU). After fine-tuning for a specific task, use case, or tenant with LoRA, the result is that the original LLM remains unchanged and a newly-trained **“LoRA adapter”** emerges. This LoRA adapter is much, much smaller than the original LLM - on the order of a single-digit % of the original LLM size (MBs vs GBs).  \n",
    "\n",
    "That said, at inference time, the LoRA adapter needs to be reunited and combined with its original LLM to serve the inference request.  The benefit, however, is that **many LoRA adapters can re-use** the original LLM which reduces overall memory requirements when serving **multiple tasks and use cases**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.1'></a>\n",
    "### 3.1 - Setup the PEFT/LoRA model for Fine-Tuning\n",
    "\n",
    "You need to set up the PEFT/LoRA model for fine-tuning with a **new layer/parameter adapter**. Using PEFT/LoRA, you are **freezing the underlying LLM** and **only training the adapter**. Have a look at the LoRA configuration below. Note the rank (`r`) hyper-parameter, which defines the rank/dimension of the adapter to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, base_model_name_or_path=None, task_type=<TaskType.SEQ_2_SEQ_LM: 'SEQ_2_SEQ_LM'>, inference_mode=False, r=32, target_modules=['q', 'v'], lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True)\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32, # Rank\n",
    "    lora_alpha=32, # LoRA scaling factor\n",
    "    target_modules=[\"q\", \"v\"], # The modules (for example, attention blocks) to apply the LoRA update matrices.\n",
    "    lora_dropout=0.05, # dropout probability, technique to prevent overfitting\n",
    "    bias=\"none\", # Specifies if the bias parameters should be trained. Can be 'none', 'all' or 'lora_only'.\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
    ")\n",
    "\n",
    "print(lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Add LoRA adapter layers/parameters to the original LLM to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.shared.weight\n",
      "24674304\n",
      "base_model.model.encoder.block.0.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.encoder.block.0.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.0.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.0.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.encoder.block.0.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.encoder.block.0.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.0.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.0.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight\n",
      "384\n",
      "base_model.model.encoder.block.0.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.0.layer.1.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.0.layer.1.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.0.layer.1.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.0.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.1.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.encoder.block.1.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.1.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.1.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.encoder.block.1.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.encoder.block.1.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.1.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.1.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.encoder.block.1.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.1.layer.1.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.1.layer.1.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.1.layer.1.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.1.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.2.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.encoder.block.2.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.2.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.2.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.encoder.block.2.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.encoder.block.2.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.2.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.2.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.encoder.block.2.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.2.layer.1.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.2.layer.1.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.2.layer.1.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.2.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.3.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.encoder.block.3.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.3.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.3.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.encoder.block.3.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.encoder.block.3.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.3.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.3.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.encoder.block.3.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.3.layer.1.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.3.layer.1.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.3.layer.1.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.3.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.4.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.encoder.block.4.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.4.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.4.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.encoder.block.4.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.encoder.block.4.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.4.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.4.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.encoder.block.4.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.4.layer.1.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.4.layer.1.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.4.layer.1.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.4.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.5.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.encoder.block.5.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.5.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.5.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.encoder.block.5.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.encoder.block.5.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.5.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.5.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.encoder.block.5.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.5.layer.1.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.5.layer.1.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.5.layer.1.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.5.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.6.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.encoder.block.6.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.6.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.6.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.encoder.block.6.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.encoder.block.6.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.6.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.6.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.encoder.block.6.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.6.layer.1.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.6.layer.1.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.6.layer.1.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.6.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.7.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.encoder.block.7.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.7.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.7.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.encoder.block.7.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.encoder.block.7.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.7.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.7.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.encoder.block.7.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.7.layer.1.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.7.layer.1.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.7.layer.1.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.7.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.8.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.encoder.block.8.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.8.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.8.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.encoder.block.8.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.encoder.block.8.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.8.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.8.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.encoder.block.8.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.8.layer.1.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.8.layer.1.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.8.layer.1.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.8.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.9.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.encoder.block.9.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.9.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.9.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.encoder.block.9.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.encoder.block.9.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.9.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.9.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.encoder.block.9.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.9.layer.1.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.9.layer.1.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.9.layer.1.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.9.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.10.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.encoder.block.10.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.10.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.10.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.encoder.block.10.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.encoder.block.10.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.10.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.10.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.encoder.block.10.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.10.layer.1.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.10.layer.1.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.10.layer.1.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.10.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.11.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.encoder.block.11.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.11.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.11.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.encoder.block.11.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.encoder.block.11.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.11.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.11.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.encoder.block.11.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.11.layer.1.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.11.layer.1.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.11.layer.1.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.11.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.final_layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.0.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.0.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.0.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.0.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.0.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.0.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.0.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.0.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight\n",
      "384\n",
      "base_model.model.decoder.block.0.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.0.layer.1.EncDecAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.0.layer.1.EncDecAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.0.layer.1.EncDecAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.0.layer.1.EncDecAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.0.layer.1.EncDecAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.0.layer.1.EncDecAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.0.layer.1.EncDecAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.0.layer.1.EncDecAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.0.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.0.layer.2.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.0.layer.2.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.0.layer.2.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.0.layer.2.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.1.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.1.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.1.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.1.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.1.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.1.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.1.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.1.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.1.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.1.layer.1.EncDecAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.1.layer.1.EncDecAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.1.layer.1.EncDecAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.1.layer.1.EncDecAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.1.layer.1.EncDecAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.1.layer.1.EncDecAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.1.layer.1.EncDecAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.1.layer.1.EncDecAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.1.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.1.layer.2.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.1.layer.2.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.1.layer.2.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.1.layer.2.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.2.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.2.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.2.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.2.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.2.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.2.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.2.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.2.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.2.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.2.layer.1.EncDecAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.2.layer.1.EncDecAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.2.layer.1.EncDecAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.2.layer.1.EncDecAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.2.layer.1.EncDecAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.2.layer.1.EncDecAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.2.layer.1.EncDecAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.2.layer.1.EncDecAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.2.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.2.layer.2.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.2.layer.2.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.2.layer.2.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.2.layer.2.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.3.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.3.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.3.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.3.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.3.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.3.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.3.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.3.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.3.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.3.layer.1.EncDecAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.3.layer.1.EncDecAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.3.layer.1.EncDecAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.3.layer.1.EncDecAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.3.layer.1.EncDecAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.3.layer.1.EncDecAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.3.layer.1.EncDecAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.3.layer.1.EncDecAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.3.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.3.layer.2.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.3.layer.2.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.3.layer.2.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.3.layer.2.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.4.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.4.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.4.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.4.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.4.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.4.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.4.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.4.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.4.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.4.layer.1.EncDecAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.4.layer.1.EncDecAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.4.layer.1.EncDecAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.4.layer.1.EncDecAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.4.layer.1.EncDecAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.4.layer.1.EncDecAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.4.layer.1.EncDecAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.4.layer.1.EncDecAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.4.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.4.layer.2.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.4.layer.2.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.4.layer.2.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.4.layer.2.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.5.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.5.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.5.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.5.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.5.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.5.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.5.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.5.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.5.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.5.layer.1.EncDecAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.5.layer.1.EncDecAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.5.layer.1.EncDecAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.5.layer.1.EncDecAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.5.layer.1.EncDecAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.5.layer.1.EncDecAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.5.layer.1.EncDecAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.5.layer.1.EncDecAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.5.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.5.layer.2.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.5.layer.2.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.5.layer.2.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.5.layer.2.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.6.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.6.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.6.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.6.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.6.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.6.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.6.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.6.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.6.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.6.layer.1.EncDecAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.6.layer.1.EncDecAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.6.layer.1.EncDecAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.6.layer.1.EncDecAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.6.layer.1.EncDecAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.6.layer.1.EncDecAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.6.layer.1.EncDecAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.6.layer.1.EncDecAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.6.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.6.layer.2.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.6.layer.2.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.6.layer.2.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.6.layer.2.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.7.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.7.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.7.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.7.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.7.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.7.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.7.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.7.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.7.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.7.layer.1.EncDecAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.7.layer.1.EncDecAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.7.layer.1.EncDecAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.7.layer.1.EncDecAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.7.layer.1.EncDecAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.7.layer.1.EncDecAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.7.layer.1.EncDecAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.7.layer.1.EncDecAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.7.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.7.layer.2.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.7.layer.2.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.7.layer.2.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.7.layer.2.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.8.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.8.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.8.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.8.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.8.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.8.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.8.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.8.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.8.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.8.layer.1.EncDecAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.8.layer.1.EncDecAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.8.layer.1.EncDecAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.8.layer.1.EncDecAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.8.layer.1.EncDecAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.8.layer.1.EncDecAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.8.layer.1.EncDecAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.8.layer.1.EncDecAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.8.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.8.layer.2.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.8.layer.2.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.8.layer.2.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.8.layer.2.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.9.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.9.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.9.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.9.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.9.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.9.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.9.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.9.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.9.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.9.layer.1.EncDecAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.9.layer.1.EncDecAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.9.layer.1.EncDecAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.9.layer.1.EncDecAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.9.layer.1.EncDecAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.9.layer.1.EncDecAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.9.layer.1.EncDecAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.9.layer.1.EncDecAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.9.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.9.layer.2.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.9.layer.2.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.9.layer.2.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.9.layer.2.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.10.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.10.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.10.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.10.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.10.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.10.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.10.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.10.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.10.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.10.layer.1.EncDecAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.10.layer.1.EncDecAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.10.layer.1.EncDecAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.10.layer.1.EncDecAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.10.layer.1.EncDecAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.10.layer.1.EncDecAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.10.layer.1.EncDecAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.10.layer.1.EncDecAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.10.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.10.layer.2.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.10.layer.2.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.10.layer.2.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.10.layer.2.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.11.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.11.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.11.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.11.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.11.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.11.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.11.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.11.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.11.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.11.layer.1.EncDecAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.11.layer.1.EncDecAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.11.layer.1.EncDecAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.11.layer.1.EncDecAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.11.layer.1.EncDecAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.11.layer.1.EncDecAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.11.layer.1.EncDecAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.11.layer.1.EncDecAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.11.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.11.layer.2.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.11.layer.2.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.11.layer.2.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.11.layer.2.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.final_layer_norm.weight\n",
      "768\n",
      "base_model.model.lm_head.weight\n",
      "24674304\n",
      "trainable model parameters: 3538944\n",
      "all model parameters: 251116800\n",
      "percentage of trainable model parameters: 1.41%\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(original_model, \n",
    "                            lora_config)\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='3.2'></a>\n",
    "### 3.2 - Train PEFT Adapter\n",
    "\n",
    "Define training arguments and create `Trainer` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./peft-dialogue-summary-training-1698669533\n"
     ]
    }
   ],
   "source": [
    "output_dir = f'./peft-dialogue-summary-training-{str(int(time.time()))}'\n",
    "print(output_dir)\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=1e-3, # Higher learning rate than full fine-tuning.\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=1,\n",
    "    max_steps=1    \n",
    ")\n",
    "    \n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now everything is ready to train the PEFT adapter and save the model.\n",
    "\n",
    "<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSIxMjUiIHZpZXdCb3g9IjAgMCA4MDAgMTI1IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogICAgPGRlZnM+CiAgICAgICAgPGxpbmVhckdyYWRpZW50IGlkPSJmYWRlR3JhZGllbnQiIHgxPSIwIiB4Mj0iMSI+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMCUiIHN0b3AtY29sb3I9IiNGMEYwRjAiLz4KICAgICAgICAgICAgPHN0b3Agb2Zmc2V0PSIxMDAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIiBzdG9wLW9wYWNpdHk9IjAiLz4KICAgICAgICA8L2xpbmVhckdyYWRpZW50PgogICAgICAgIDxtYXNrIGlkPSJmYWRlTWFzayI+CiAgICAgICAgICAgIDxyZWN0IHg9IjAiIHk9IjAiIHdpZHRoPSI3NTAiIGhlaWdodD0iMTI1IiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSIxMjUiIGZpbGw9InVybCgjZmFkZUdyYWRpZW50KSIvPgogICAgICAgIDwvbWFzaz4KICAgIDwvZGVmcz4KICAgIDxwYXRoIGQ9Ik0zLDUwIEE1MCw1MCAwIDAgMSA1MywzIEw3OTcsMyBMNzk3LDk3IEw5Nyw5NyBMNTAsMTE1IEwzLDk3IFoiIGZpbGw9IiNGMEYwRjAiIHN0cm9rZT0iI0UwRTBFMCIgc3Ryb2tlLXdpZHRoPSIxIiBtYXNrPSJ1cmwoI2ZhZGVNYXNrKSIvPgogICAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgcj0iMzAiIGZpbGw9IiM1N2M0ZjgiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIxIi8+CiAgICA8Y2lyY2xlIGN4PSI1MCIgY3k9IjUwIiByPSIyNSIgZmlsbD0iI0YwRjBGMCIvPgogICAgPGxpbmUgeDE9IjUwIiB5MT0iNTAiIHgyPSI1MCIgeTI9IjMwIiBzdHJva2U9IiM1N2M0ZjgiIHN0cm9rZS13aWR0aD0iMyIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIi8+CiAgICA8bGluZSB4MT0iNTAiIHkxPSI1MCIgeDI9IjY1IiB5Mj0iNTAiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIzIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KICAgIDx0ZXh0IHg9IjEwMCIgeT0iMzQiIGZvbnQtZmFtaWx5PSJBcmlhbCwgc2Fucy1zZXJpZiIgZm9udC1zaXplPSIxNCIgZmlsbD0iIzMzMzMzMyI+VGhlIG5leHQgY2VsbCBtYXkgdGFrZSBhIGZldyBtaW51dGVzIHRvIHJ1bi48L3RleHQ+Cjwvc3ZnPgo=\" alt=\"Time alert open medium\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:03, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>46.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./peft-dialogue-summary-checkpoint-local/tokenizer_config.json',\n",
       " './peft-dialogue-summary-checkpoint-local/special_tokens_map.json',\n",
       " './peft-dialogue-summary-checkpoint-local/tokenizer.json')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_trainer.train()\n",
    "\n",
    "#peft_model_path=\"./peft-dialogue-summary-checkpoint-local\"\n",
    "peft_model_path = output_dir\n",
    "\n",
    "peft_trainer.model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSI1MCIgdmlld0JveD0iMCAwIDgwMCA1MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxkZWZzPgogICAgICAgIDxsaW5lYXJHcmFkaWVudCBpZD0iZmFkZUdyYWRpZW50IiB4MT0iMCIgeDI9IjEiPgogICAgICAgICAgICA8c3RvcCBvZmZzZXQ9IjAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIi8+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMTAwJSIgc3RvcC1jb2xvcj0iI0YwRjBGMCIgc3RvcC1vcGFjaXR5PSIwIi8+CiAgICAgICAgPC9saW5lYXJHcmFkaWVudD4KICAgICAgICA8bWFzayBpZD0iZmFkZU1hc2siPgogICAgICAgICAgICA8cmVjdCB4PSIwIiB5PSIwIiB3aWR0aD0iNzUwIiBoZWlnaHQ9IjUwIiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSI1MCIgZmlsbD0idXJsKCNmYWRlR3JhZGllbnQpIi8+CiAgICAgICAgPC9tYXNrPgogICAgPC9kZWZzPgogICAgPHBhdGggZD0iTTI1LDUwIFEwLDUwIDAsMjUgTDUwLDMgTDk3LDI1IEw3OTcsMjUgTDc5Nyw1MCBMMjUsNTAgWiIgZmlsbD0iI0YwRjBGMCIgc3Ryb2tlPSIjRTBFMEUwIiBzdHJva2Utd2lkdGg9IjEiIG1hc2s9InVybCgjZmFkZU1hc2spIi8+Cjwvc3ZnPgo=\" alt=\"Time alert close\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "That training was performed on a subset of data. To load a fully trained PEFT model, read a checkpoint of a PEFT model from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://dlai-generative-ai/models/peft-dialogue-summary-checkpoint/adapter_config.json to peft-dialogue-summary-checkpoint-from-s3/adapter_config.json\n",
      "download: s3://dlai-generative-ai/models/peft-dialogue-summary-checkpoint/special_tokens_map.json to peft-dialogue-summary-checkpoint-from-s3/special_tokens_map.json\n",
      "download: s3://dlai-generative-ai/models/peft-dialogue-summary-checkpoint/tokenizer_config.json to peft-dialogue-summary-checkpoint-from-s3/tokenizer_config.json\n",
      "download: s3://dlai-generative-ai/models/peft-dialogue-summary-checkpoint/tokenizer.json to peft-dialogue-summary-checkpoint-from-s3/tokenizer.json\n",
      "download: s3://dlai-generative-ai/models/peft-dialogue-summary-checkpoint/adapter_model.bin to peft-dialogue-summary-checkpoint-from-s3/adapter_model.bin\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive s3://dlai-generative-ai/models/peft-dialogue-summary-checkpoint/ ./peft-dialogue-summary-checkpoint-from-s3/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Check that the size of this model is much less than the original LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 14208525 May 15 11:18 ./peft-dialogue-summary-checkpoint-from-s3/adapter_model.bin\n"
     ]
    }
   ],
   "source": [
    "!ls -al ./peft-dialogue-summary-checkpoint-from-s3/adapter_model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "-rwxrwxrwx 1 root root 14206477 Oct 30 12:54 ./peft-dialogue-summary-training-1698669533/adapter_model.bin\n"
     ]
    }
   ],
   "source": [
    "!ls -al ./peft-dialogue-summary-training-1698669533/adapter_model.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Prepare this model by adding an adapter to the original FLAN-T5 model. You are setting `is_trainable=False` because the plan is only to perform inference with this PEFT model. If you were preparing the model for further training, you would set `is_trainable=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSeq2SeqLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): T5ForConditionalGeneration(\n",
       "      (shared): Embedding(32128, 768)\n",
       "      (encoder): T5Stack(\n",
       "        (embed_tokens): Embedding(32128, 768)\n",
       "        (block): ModuleList(\n",
       "          (0): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (relative_attention_bias): Embedding(32, 12)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (4): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (5): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (6): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (7): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (8): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (9): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (10): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (11): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): T5LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (decoder): T5Stack(\n",
       "        (embed_tokens): Embedding(32128, 768)\n",
       "        (block): ModuleList(\n",
       "          (0): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (relative_attention_bias): Embedding(32, 12)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (4): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (5): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (6): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (7): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (8): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (9): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (10): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (11): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=768, out_features=768, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): T5LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\", torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "# peft_model = PeftModel.from_pretrained(peft_model_base,\n",
    "#                                        './peft-dialogue-summary-checkpoint-from-s3/',\n",
    "#                                        torch_dtype=torch.bfloat16,\n",
    "#                                        is_trainable=False)\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(model=peft_model_base, # ?\n",
    "                                       #model_id='./peft-dialogue-summary-checkpoint-from-s3/', #?\n",
    "                                       model_id=output_dir,\n",
    "                                       torch_dtype=torch.bfloat16,\n",
    "                                       is_trainable=False)\n",
    "\n",
    "peft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The number of trainable parameters will be `0` due to `is_trainable=False` setting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.shared.weight\n",
      "24674304\n",
      "base_model.model.encoder.block.0.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.encoder.block.0.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.0.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.0.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.encoder.block.0.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.encoder.block.0.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.0.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.0.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight\n",
      "384\n",
      "base_model.model.encoder.block.0.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.0.layer.1.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.0.layer.1.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.0.layer.1.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.0.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.1.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.encoder.block.1.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.1.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.1.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.encoder.block.1.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.encoder.block.1.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.1.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.1.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.encoder.block.1.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.1.layer.1.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.1.layer.1.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.1.layer.1.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.1.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.2.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.encoder.block.2.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.2.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.2.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.encoder.block.2.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.encoder.block.2.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.2.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.2.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.encoder.block.2.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.2.layer.1.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.2.layer.1.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.2.layer.1.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.2.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.3.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.encoder.block.3.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.3.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.3.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.encoder.block.3.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.encoder.block.3.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.3.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.3.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.encoder.block.3.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.3.layer.1.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.3.layer.1.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.3.layer.1.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.3.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.4.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.encoder.block.4.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.4.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.4.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.encoder.block.4.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.encoder.block.4.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.4.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.4.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.encoder.block.4.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.4.layer.1.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.4.layer.1.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.4.layer.1.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.4.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.5.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.encoder.block.5.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.5.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.5.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.encoder.block.5.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.encoder.block.5.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.5.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.5.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.encoder.block.5.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.5.layer.1.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.5.layer.1.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.5.layer.1.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.5.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.6.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.encoder.block.6.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.6.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.6.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.encoder.block.6.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.encoder.block.6.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.6.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.6.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.encoder.block.6.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.6.layer.1.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.6.layer.1.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.6.layer.1.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.6.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.7.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.encoder.block.7.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.7.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.7.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.encoder.block.7.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.encoder.block.7.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.7.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.7.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.encoder.block.7.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.7.layer.1.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.7.layer.1.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.7.layer.1.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.7.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.8.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.encoder.block.8.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.8.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.8.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.encoder.block.8.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.encoder.block.8.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.8.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.8.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.encoder.block.8.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.8.layer.1.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.8.layer.1.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.8.layer.1.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.8.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.9.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.encoder.block.9.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.9.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.9.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.encoder.block.9.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.encoder.block.9.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.9.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.9.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.encoder.block.9.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.9.layer.1.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.9.layer.1.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.9.layer.1.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.9.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.10.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.encoder.block.10.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.10.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.10.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.encoder.block.10.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.encoder.block.10.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.10.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.10.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.encoder.block.10.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.10.layer.1.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.10.layer.1.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.10.layer.1.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.10.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.11.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.encoder.block.11.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.11.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.11.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.encoder.block.11.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.encoder.block.11.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.11.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.encoder.block.11.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.encoder.block.11.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.block.11.layer.1.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.11.layer.1.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.11.layer.1.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.encoder.block.11.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.encoder.final_layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.0.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.0.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.0.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.0.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.0.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.0.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.0.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.0.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight\n",
      "384\n",
      "base_model.model.decoder.block.0.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.0.layer.1.EncDecAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.0.layer.1.EncDecAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.0.layer.1.EncDecAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.0.layer.1.EncDecAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.0.layer.1.EncDecAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.0.layer.1.EncDecAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.0.layer.1.EncDecAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.0.layer.1.EncDecAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.0.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.0.layer.2.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.0.layer.2.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.0.layer.2.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.0.layer.2.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.1.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.1.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.1.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.1.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.1.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.1.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.1.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.1.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.1.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.1.layer.1.EncDecAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.1.layer.1.EncDecAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.1.layer.1.EncDecAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.1.layer.1.EncDecAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.1.layer.1.EncDecAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.1.layer.1.EncDecAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.1.layer.1.EncDecAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.1.layer.1.EncDecAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.1.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.1.layer.2.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.1.layer.2.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.1.layer.2.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.1.layer.2.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.2.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.2.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.2.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.2.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.2.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.2.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.2.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.2.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.2.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.2.layer.1.EncDecAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.2.layer.1.EncDecAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.2.layer.1.EncDecAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.2.layer.1.EncDecAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.2.layer.1.EncDecAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.2.layer.1.EncDecAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.2.layer.1.EncDecAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.2.layer.1.EncDecAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.2.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.2.layer.2.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.2.layer.2.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.2.layer.2.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.2.layer.2.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.3.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.3.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.3.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.3.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.3.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.3.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.3.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.3.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.3.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.3.layer.1.EncDecAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.3.layer.1.EncDecAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.3.layer.1.EncDecAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.3.layer.1.EncDecAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.3.layer.1.EncDecAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.3.layer.1.EncDecAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.3.layer.1.EncDecAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.3.layer.1.EncDecAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.3.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.3.layer.2.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.3.layer.2.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.3.layer.2.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.3.layer.2.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.4.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.4.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.4.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.4.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.4.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.4.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.4.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.4.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.4.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.4.layer.1.EncDecAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.4.layer.1.EncDecAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.4.layer.1.EncDecAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.4.layer.1.EncDecAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.4.layer.1.EncDecAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.4.layer.1.EncDecAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.4.layer.1.EncDecAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.4.layer.1.EncDecAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.4.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.4.layer.2.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.4.layer.2.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.4.layer.2.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.4.layer.2.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.5.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.5.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.5.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.5.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.5.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.5.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.5.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.5.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.5.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.5.layer.1.EncDecAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.5.layer.1.EncDecAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.5.layer.1.EncDecAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.5.layer.1.EncDecAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.5.layer.1.EncDecAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.5.layer.1.EncDecAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.5.layer.1.EncDecAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.5.layer.1.EncDecAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.5.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.5.layer.2.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.5.layer.2.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.5.layer.2.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.5.layer.2.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.6.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.6.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.6.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.6.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.6.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.6.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.6.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.6.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.6.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.6.layer.1.EncDecAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.6.layer.1.EncDecAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.6.layer.1.EncDecAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.6.layer.1.EncDecAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.6.layer.1.EncDecAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.6.layer.1.EncDecAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.6.layer.1.EncDecAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.6.layer.1.EncDecAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.6.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.6.layer.2.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.6.layer.2.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.6.layer.2.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.6.layer.2.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.7.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.7.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.7.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.7.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.7.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.7.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.7.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.7.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.7.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.7.layer.1.EncDecAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.7.layer.1.EncDecAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.7.layer.1.EncDecAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.7.layer.1.EncDecAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.7.layer.1.EncDecAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.7.layer.1.EncDecAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.7.layer.1.EncDecAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.7.layer.1.EncDecAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.7.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.7.layer.2.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.7.layer.2.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.7.layer.2.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.7.layer.2.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.8.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.8.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.8.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.8.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.8.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.8.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.8.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.8.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.8.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.8.layer.1.EncDecAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.8.layer.1.EncDecAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.8.layer.1.EncDecAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.8.layer.1.EncDecAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.8.layer.1.EncDecAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.8.layer.1.EncDecAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.8.layer.1.EncDecAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.8.layer.1.EncDecAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.8.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.8.layer.2.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.8.layer.2.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.8.layer.2.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.8.layer.2.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.9.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.9.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.9.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.9.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.9.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.9.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.9.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.9.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.9.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.9.layer.1.EncDecAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.9.layer.1.EncDecAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.9.layer.1.EncDecAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.9.layer.1.EncDecAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.9.layer.1.EncDecAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.9.layer.1.EncDecAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.9.layer.1.EncDecAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.9.layer.1.EncDecAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.9.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.9.layer.2.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.9.layer.2.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.9.layer.2.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.9.layer.2.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.10.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.10.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.10.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.10.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.10.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.10.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.10.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.10.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.10.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.10.layer.1.EncDecAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.10.layer.1.EncDecAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.10.layer.1.EncDecAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.10.layer.1.EncDecAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.10.layer.1.EncDecAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.10.layer.1.EncDecAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.10.layer.1.EncDecAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.10.layer.1.EncDecAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.10.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.10.layer.2.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.10.layer.2.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.10.layer.2.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.10.layer.2.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.11.layer.0.SelfAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.11.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.11.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.11.layer.0.SelfAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.11.layer.0.SelfAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.11.layer.0.SelfAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.11.layer.0.SelfAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.11.layer.0.SelfAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.11.layer.0.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.11.layer.1.EncDecAttention.q.weight\n",
      "589824\n",
      "base_model.model.decoder.block.11.layer.1.EncDecAttention.q.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.11.layer.1.EncDecAttention.q.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.11.layer.1.EncDecAttention.k.weight\n",
      "589824\n",
      "base_model.model.decoder.block.11.layer.1.EncDecAttention.v.weight\n",
      "589824\n",
      "base_model.model.decoder.block.11.layer.1.EncDecAttention.v.lora_A.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.11.layer.1.EncDecAttention.v.lora_B.default.weight\n",
      "24576\n",
      "base_model.model.decoder.block.11.layer.1.EncDecAttention.o.weight\n",
      "589824\n",
      "base_model.model.decoder.block.11.layer.1.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.block.11.layer.2.DenseReluDense.wi_0.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.11.layer.2.DenseReluDense.wi_1.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.11.layer.2.DenseReluDense.wo.weight\n",
      "1572864\n",
      "base_model.model.decoder.block.11.layer.2.layer_norm.weight\n",
      "768\n",
      "base_model.model.decoder.final_layer_norm.weight\n",
      "768\n",
      "base_model.model.lm_head.weight\n",
      "24674304\n",
      "trainable model parameters: 0\n",
      "all model parameters: 251116800\n",
      "percentage of trainable model parameters: 0.00%\n"
     ]
    }
   ],
   "source": [
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.3'></a>\n",
    "### 3.3 - Evaluate the Model Qualitatively (Human Evaluation)\n",
    "\n",
    "Make inferences for the same example as in sections [1.3](#1.3) and [2.3](#2.3), with the original model, fully fine-tuned and PEFT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "ORIGINAL MODEL:\n",
      "#Person1#: I'm not sure what I would need. #Person2#: I'm not sure what exactly I would need. #Person1#: I could also consider adding a painting program to your software. #Person1#: I'm not sure what exactly I would need. #Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for your business. #Person1#: That would be a definite bonus. #Person2#: You might also want to add a CD-ROM drive too. #Person1#: I'm not sure what I would need.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INSTRUCT MODEL:\n",
      "#Person1#: I'm thinking of upgrading my computer.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "PEFT MODEL: #Person1#: You can upgrade your system. #Person2#: I'm not sure what exactly you want to do. #Person1#: I'm not sure what you want to do. #Person2#: I'm not sure what you want to do. #Person1#: I'm not sure what you want to do. #Person2#: I'm not sure what I want to do. #Person2#: I'm not sure what I want to do. #Person1#: I'm not sure what I want to do. #Person2#: I'm not sure what I want to do. #Person1#: I'm not sure what I want to do. #Person2#: I'm not sure what I want to do. #Person1#: I'm not sure what I want to do.\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "baseline_human_summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary: \"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n",
    "print(dash_line)\n",
    "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'PEFT MODEL: {peft_model_text_output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.4'></a>\n",
    "### 3.4 - Evaluate the Model Quantitatively (with ROUGE Metric)\n",
    "Perform inferences for the sample of the test dataset (only 10 dialogues and summaries to save time). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_baseline_summaries</th>\n",
       "      <th>original_model_summaries</th>\n",
       "      <th>instruct_model_summaries</th>\n",
       "      <th>peft_model_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n",
       "      <td>Employees are required to take a dictation to ...</td>\n",
       "      <td>#Person1#: I need to take a dictation for you.</td>\n",
       "      <td>This memo is intended to be an intra-office memo.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In order to prevent employees from wasting tim...</td>\n",
       "      <td>The memo is being prepared for the intra-offic...</td>\n",
       "      <td>#Person1#: I need to take a dictation for you.</td>\n",
       "      <td>Employees will receive a memo from the Office ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n",
       "      <td>This memo is for internal and external communi...</td>\n",
       "      <td>#Person1#: I need to take a dictation for you.</td>\n",
       "      <td>#Person1#: Ms. Dawson, I need to take a dictat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Person2# arrives late because of traffic jam....</td>\n",
       "      <td>The driver is a bit nervous.</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "      <td>#Person2: I'm finally here. #Person1: I'm sorr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n",
       "      <td>People have been talking about the traffic jam...</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "      <td>The conversation is about a car accident.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#Person2# complains to #Person1# about the tra...</td>\n",
       "      <td>#Porner1: You're finally here! #Person2: You'r...</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "      <td>The conversation is about the traffic situatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are divorced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#Person1# and Kate talk about the divorce betw...</td>\n",
       "      <td>The divorce is a very personal decision.</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting a divorce.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#Person1# and Brian are at the birthday party ...</td>\n",
       "      <td>#Person1#: Thank you for coming to the party. ...</td>\n",
       "      <td>#Person1#: Happy birthday, Brian. #Person2#: I...</td>\n",
       "      <td>Brian's birthday is coming up.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            human_baseline_summaries  \\\n",
       "0  Ms. Dawson helps #Person1# to write a memo to ...   \n",
       "1  In order to prevent employees from wasting tim...   \n",
       "2  Ms. Dawson takes a dictation for #Person1# abo...   \n",
       "3  #Person2# arrives late because of traffic jam....   \n",
       "4  #Person2# decides to follow #Person1#'s sugges...   \n",
       "5  #Person2# complains to #Person1# about the tra...   \n",
       "6  #Person1# tells Kate that Masha and Hero get d...   \n",
       "7  #Person1# tells Kate that Masha and Hero are g...   \n",
       "8  #Person1# and Kate talk about the divorce betw...   \n",
       "9  #Person1# and Brian are at the birthday party ...   \n",
       "\n",
       "                            original_model_summaries  \\\n",
       "0  Employees are required to take a dictation to ...   \n",
       "1  The memo is being prepared for the intra-offic...   \n",
       "2  This memo is for internal and external communi...   \n",
       "3                       The driver is a bit nervous.   \n",
       "4  People have been talking about the traffic jam...   \n",
       "5  #Porner1: You're finally here! #Person2: You'r...   \n",
       "6               Masha and Hero are getting divorced.   \n",
       "7               Masha and Hero are getting divorced.   \n",
       "8           The divorce is a very personal decision.   \n",
       "9  #Person1#: Thank you for coming to the party. ...   \n",
       "\n",
       "                            instruct_model_summaries  \\\n",
       "0     #Person1#: I need to take a dictation for you.   \n",
       "1     #Person1#: I need to take a dictation for you.   \n",
       "2     #Person1#: I need to take a dictation for you.   \n",
       "3  The traffic jam at the Carrefour intersection ...   \n",
       "4  The traffic jam at the Carrefour intersection ...   \n",
       "5  The traffic jam at the Carrefour intersection ...   \n",
       "6               Masha and Hero are getting divorced.   \n",
       "7               Masha and Hero are getting divorced.   \n",
       "8               Masha and Hero are getting divorced.   \n",
       "9  #Person1#: Happy birthday, Brian. #Person2#: I...   \n",
       "\n",
       "                                peft_model_summaries  \n",
       "0  This memo is intended to be an intra-office memo.  \n",
       "1  Employees will receive a memo from the Office ...  \n",
       "2  #Person1#: Ms. Dawson, I need to take a dictat...  \n",
       "3  #Person2: I'm finally here. #Person1: I'm sorr...  \n",
       "4          The conversation is about a car accident.  \n",
       "5  The conversation is about the traffic situatio...  \n",
       "6               Masha and Hero are getting divorced.  \n",
       "7                       Masha and Hero are divorced.  \n",
       "8              Masha and Hero are getting a divorce.  \n",
       "9                     Brian's birthday is coming up.  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogues = dataset['test'][0:10]['dialogue']\n",
    "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
    "\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "peft_model_summaries = []\n",
    "\n",
    "for idx, dialogue in enumerate(dialogues):\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary: \"\"\"\n",
    "    \n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    human_baseline_text_output = human_baseline_summaries[idx]\n",
    "    \n",
    "    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "    instruct_model_summaries.append(instruct_model_text_output)\n",
    "    peft_model_summaries.append(peft_model_text_output)\n",
    "\n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries, peft_model_summaries))\n",
    " \n",
    "df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries', 'peft_model_summaries'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute ROUGE score for this subset of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.18386679635691494, 'rouge2': 0.06666666666666667, 'rougeL': 0.16831507535064846, 'rougeLsum': 0.17073404197910125}\n",
      "INSTRUCT MODEL:\n",
      "{'rouge1': 0.24089921652421653, 'rouge2': 0.11769053708439897, 'rougeL': 0.22001958689458687, 'rougeLsum': 0.22134175465057818}\n",
      "PEFT MODEL:\n",
      "{'rouge1': 0.23904636048685296, 'rouge2': 0.0965485859895798, 'rougeL': 0.214697694078679, 'rougeLsum': 0.2180915204000019}\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions=instruct_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "peft_model_results = rouge.compute(\n",
    "    predictions=peft_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)\n",
    "print('PEFT MODEL:')\n",
    "print(peft_model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, that PEFT model results are not too bad, while the training process was much easier!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You already computed ROUGE score on the full dataset, after loading the results from the `data/dialogue-summary-training-results.csv` file. Load the values for the PEFT model now and check its performance compared to other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.2334158581572823, 'rouge2': 0.07603964187010573, 'rougeL': 0.20145520923859048, 'rougeLsum': 0.20145899339006135}\n",
      "INSTRUCT MODEL:\n",
      "{'rouge1': 0.42161291557556113, 'rouge2': 0.18035380596301792, 'rougeL': 0.3384439349963909, 'rougeLsum': 0.33835653595561666}\n",
      "PEFT MODEL:\n",
      "{'rouge1': 0.40810631575616746, 'rouge2': 0.1633255794568712, 'rougeL': 0.32507074586565354, 'rougeLsum': 0.3248950182867091}\n"
     ]
    }
   ],
   "source": [
    "human_baseline_summaries = results['human_baseline_summaries'].values\n",
    "original_model_summaries = results['original_model_summaries'].values\n",
    "instruct_model_summaries = results['instruct_model_summaries'].values\n",
    "peft_model_summaries     = results['peft_model_summaries'].values\n",
    "\n",
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions=instruct_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "peft_model_results = rouge.compute(\n",
    "    predictions=peft_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)\n",
    "print('PEFT MODEL:')\n",
    "print(peft_model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show less of an improvement over full fine-tuning, but the benefits of PEFT typically outweigh the slightly-lower performance metrics.\n",
    "\n",
    "Calculate the improvement of PEFT over the original model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute percentage improvement of PEFT MODEL over HUMAN BASELINE\n",
      "rouge1: 17.47%\n",
      "rouge2: 8.73%\n",
      "rougeL: 12.36%\n",
      "rougeLsum: 12.34%\n"
     ]
    }
   ],
   "source": [
    "print(\"Absolute percentage improvement of PEFT MODEL over HUMAN BASELINE\")\n",
    "\n",
    "improvement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\n",
    "for key, value in zip(peft_model_results.keys(), improvement):\n",
    "    print(f'{key}: {value*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate the improvement of PEFT over a full fine-tuned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute percentage improvement of PEFT MODEL over INSTRUCT MODEL\n",
      "rouge1: -1.35%\n",
      "rouge2: -1.70%\n",
      "rougeL: -1.34%\n",
      "rougeLsum: -1.35%\n"
     ]
    }
   ],
   "source": [
    "print(\"Absolute percentage improvement of PEFT MODEL over INSTRUCT MODEL\")\n",
    "\n",
    "improvement = (np.array(list(peft_model_results.values())) - np.array(list(instruct_model_results.values())))\n",
    "for key, value in zip(peft_model_results.keys(), improvement):\n",
    "    print(f'{key}: {value*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you see a small percentage decrease in the ROUGE metrics vs. full fine-tuned. However, the training requires much less computing and memory resources (often just a single GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "colab": {
   "name": "Fine-tune a language model",
   "provenance": []
  },
  "instance_type": "ml.m5.2xlarge",
  "kernelspec": {
   "display_name": "azureml_py310_sdkv2_generative_ai_with_llms",
   "language": "python",
   "name": "azureml_py310_sdkv2_generative_ai_with_llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
